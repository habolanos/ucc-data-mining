{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d458ef-c099-4888-bfb8-342e5a3cde58",
   "metadata": {},
   "source": [
    "# Taller Dataminig – Deep Learning\r",
    "Basado en la siguiente estructura de datos de un archivo .csv, realizar los siguientes ejercicios de DeepLearning \n",
    "en el lenguaje python y librerias como Scikit Learn, Keras, Shap, Pytorch:\n",
    " \r\n",
    "CASE#,DATE OF OCCURRENCE,BLOCK, IUCR, PRIMARY DESCRIPTION, SECONDARY DESCRIPTION, LOCATIO \r\n",
    "DESCRIPTION,ARREST,DOMESTIC,BEAT,WARD,FBI CD,X COORDINATE,Y COORDINATE,LATITUDE,LONGITUDE,LOCAT\n",
    "ION\r\n",
    "JG406115,08/31/2023 07:00:00 PM,042XX W MARQUETTE RD,0498,BATTERY,\"AGG. DOMESTIC BATTERY - HANDS, FISTS, FEET, SER US \r\n",
    "INJURY\",APARTMENT,Y,Y,833,23,04B,1149062,1859830,41.771296232,-87.729149311,\"(41.771296232, -87.72914931\n",
    "\n",
    "Archivo de datos en repo ucc-datamining: ucc-data-mining/taller-dl/data.csv\r\n",
    "1)\".\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfa62e-6ddc-43cf-8f4c-534030548812",
   "metadata": {},
   "source": [
    "## 1. Predicción de Clasificación Temporal con Redes Neuronales Recurrentes (RNN)\r",
    "*  Este ejercicio implica predecir la ocurrencia de un cierto tipo de crimen en\r\n",
    "función de la fecha y hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "236d794e-a625-4d9a-ad3a-cf6100e92a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\informatica.cal\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.2151 - loss: 2.4176 - val_accuracy: 0.2221 - val_loss: 2.3429\n",
      "Epoch 2/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2255 - loss: 2.3422 - val_accuracy: 0.2278 - val_loss: 2.3372\n",
      "Epoch 3/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.2255 - loss: 2.3405 - val_accuracy: 0.2293 - val_loss: 2.3349\n",
      "Epoch 4/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.2278 - loss: 2.3379 - val_accuracy: 0.2289 - val_loss: 2.3323\n",
      "Epoch 5/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - accuracy: 0.2286 - loss: 2.3330 - val_accuracy: 0.2293 - val_loss: 2.3266\n",
      "Epoch 6/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2286 - loss: 2.3306 - val_accuracy: 0.2251 - val_loss: 2.3310\n",
      "Epoch 7/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.2294 - loss: 2.3317 - val_accuracy: 0.2294 - val_loss: 2.3250\n",
      "Epoch 8/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2297 - loss: 2.3269 - val_accuracy: 0.2279 - val_loss: 2.3279\n",
      "Epoch 9/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.2291 - loss: 2.3250 - val_accuracy: 0.2307 - val_loss: 2.3231\n",
      "Epoch 10/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2300 - loss: 2.3266 - val_accuracy: 0.2313 - val_loss: 2.3241\n",
      "Epoch 11/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2320 - loss: 2.3239 - val_accuracy: 0.2296 - val_loss: 2.3236\n",
      "Epoch 12/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.2297 - loss: 2.3276 - val_accuracy: 0.2298 - val_loss: 2.3229\n",
      "Epoch 13/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.2308 - loss: 2.3249 - val_accuracy: 0.2315 - val_loss: 2.3221\n",
      "Epoch 14/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2307 - loss: 2.3248 - val_accuracy: 0.2314 - val_loss: 2.3225\n",
      "Epoch 15/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.2302 - loss: 2.3282 - val_accuracy: 0.2306 - val_loss: 2.3224\n",
      "Epoch 16/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.2303 - loss: 2.3291 - val_accuracy: 0.2306 - val_loss: 2.3225\n",
      "Epoch 17/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.2305 - loss: 2.3251 - val_accuracy: 0.2318 - val_loss: 2.3209\n",
      "Epoch 18/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.2310 - loss: 2.3208 - val_accuracy: 0.2313 - val_loss: 2.3232\n",
      "Epoch 19/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.2312 - loss: 2.3203 - val_accuracy: 0.2320 - val_loss: 2.3212\n",
      "Epoch 20/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.2311 - loss: 2.3234 - val_accuracy: 0.2314 - val_loss: 2.3210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18fb25922d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('data.csv')\n",
    "# Preprocesamiento de datos\n",
    "\n",
    "data['DATE OF OCCURRENCE'] = pd.to_datetime(data['DATE OF OCCURRENCE'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "# data['DATE OF OCCURRENCE'] = pd.to_datetime(data['DATE OF OCCURRENCE'])\n",
    "\n",
    "data['HOUR'] = data['DATE OF OCCURRENCE'].dt.hour\n",
    "data['DAY_OF_WEEK'] = data['DATE OF OCCURRENCE'].dt.dayofweek\n",
    "data['MONTH'] = data['DATE OF OCCURRENCE'].dt.month\n",
    "# Seleccionar características y etiquetas\n",
    "X = data[['HOUR', 'DAY_OF_WEEK', 'MONTH']].values\n",
    "y = data['PRIMARY DESCRIPTION']\n",
    "# Codificar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Construir la red neuronal recurrente (LSTM)\n",
    "model = Sequential([\n",
    " LSTM(64, input_shape=(X_train.shape[1], 1), activation='relu', return_sequences=True),\n",
    " LSTM(32, activation='relu'),\n",
    " Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Ajustar el modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac92ab-15db-4de3-9bc8-799eb077f2f6",
   "metadata": {},
   "source": [
    "## 2. Predicción de Series Temporales con Redes Neuronales Convolucionales (CNN)\r",
    "*  Este ejercicio implica predecir la ocurrencia de un tipo de crimen utilizando datos\r\n",
    "de series temporales como entrada a una CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd27a683-6866-4369-9a35-b03ef106d810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.2184 - loss: 2.4204 - val_accuracy: 0.2205 - val_loss: 2.3793\n",
      "Epoch 2/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2227 - loss: 2.3685 - val_accuracy: 0.2205 - val_loss: 2.3818\n",
      "Epoch 3/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2221 - loss: 2.3663 - val_accuracy: 0.2205 - val_loss: 2.3794\n",
      "Epoch 4/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2244 - loss: 2.3623 - val_accuracy: 0.2205 - val_loss: 2.3803\n",
      "Epoch 5/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2211 - loss: 2.3680 - val_accuracy: 0.2205 - val_loss: 2.3824\n",
      "Epoch 6/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2222 - loss: 2.3670 - val_accuracy: 0.2205 - val_loss: 2.3750\n",
      "Epoch 7/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2200 - loss: 2.3665 - val_accuracy: 0.2205 - val_loss: 2.3822\n",
      "Epoch 8/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.2218 - loss: 2.3631 - val_accuracy: 0.2205 - val_loss: 2.3840\n",
      "Epoch 9/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2216 - loss: 2.3663 - val_accuracy: 0.2205 - val_loss: 2.3918\n",
      "Epoch 10/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2222 - loss: 2.3623 - val_accuracy: 0.2205 - val_loss: 2.3806\n",
      "Epoch 11/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2230 - loss: 2.3639 - val_accuracy: 0.2205 - val_loss: 2.3798\n",
      "Epoch 12/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.2208 - loss: 2.3651 - val_accuracy: 0.2205 - val_loss: 2.3808\n",
      "Epoch 13/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2214 - loss: 2.3640 - val_accuracy: 0.2205 - val_loss: 2.3802\n",
      "Epoch 14/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.2213 - loss: 2.3643 - val_accuracy: 0.2205 - val_loss: 2.3846\n",
      "Epoch 15/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2220 - loss: 2.3631 - val_accuracy: 0.2205 - val_loss: 2.3799\n",
      "Epoch 16/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2224 - loss: 2.3630 - val_accuracy: 0.2205 - val_loss: 2.3830\n",
      "Epoch 17/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - accuracy: 0.2218 - loss: 2.3626 - val_accuracy: 0.2205 - val_loss: 2.3794\n",
      "Epoch 18/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2218 - loss: 2.3616 - val_accuracy: 0.2205 - val_loss: 2.3777\n",
      "Epoch 19/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.2226 - loss: 2.3627 - val_accuracy: 0.2205 - val_loss: 2.3821\n",
      "Epoch 20/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.2221 - loss: 2.3625 - val_accuracy: 0.2205 - val_loss: 2.3808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18fb9315ed0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "# Asegúrate de que los datos estén ordenados cronológicamente\n",
    "data['DATE OF OCCURRENCE'] = pd.to_datetime(data['DATE OF OCCURRENCE'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "# data['DATE OF OCCURRENCE'] = pd.to_datetime(data['DATE OF OCCURRENCE'])\n",
    "data = data.sort_values(by='DATE OF OCCURRENCE')\n",
    "\n",
    "# Seleccionar características y etiquetas\n",
    "X = data[['DATE OF OCCURRENCE']].values\n",
    "y = data['PRIMARY DESCRIPTION']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Construir el modelo\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(len(y.unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## Ajustar etiquetas:\n",
    "# Convertir las etiquetas a números enteros\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Ajustar el modelo\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "model.fit(X_train, y_train_encoded, epochs=20, batch_size=32, validation_data=(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019803bd-bcee-4524-8382-90bd19b4abba",
   "metadata": {},
   "source": [
    "## 3. Predicción de Series Temporales con Redes Neuronales Auto-Recurrentes (ARNN)\r",
    "*  Este ejercicio implica predecir la ocurrencia de un tipo de crimen utilizando datos\r\n",
    "de series temporales y una red neuronal auto-recurrent.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c3ad0e4-13c2-4d79-93b8-13f28d158484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.2136 - loss: 2.4299 - val_accuracy: 0.2205 - val_loss: 2.3914\n",
      "Epoch 2/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2225 - loss: 2.3653 - val_accuracy: 0.2205 - val_loss: 2.3800\n",
      "Epoch 3/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2196 - loss: 2.3699 - val_accuracy: 0.2205 - val_loss: 2.3878\n",
      "Epoch 4/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2226 - loss: 2.3665 - val_accuracy: 0.2205 - val_loss: 2.3785\n",
      "Epoch 5/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2214 - loss: 2.3668 - val_accuracy: 0.2205 - val_loss: 2.3903\n",
      "Epoch 6/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2202 - loss: 2.3664 - val_accuracy: 0.2205 - val_loss: 2.3825\n",
      "Epoch 7/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2210 - loss: 2.3666 - val_accuracy: 0.2205 - val_loss: 2.3831\n",
      "Epoch 8/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2210 - loss: 2.3639 - val_accuracy: 0.2205 - val_loss: 2.3776\n",
      "Epoch 9/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2210 - loss: 2.3627 - val_accuracy: 0.2205 - val_loss: 2.3818\n",
      "Epoch 10/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2208 - loss: 2.3633 - val_accuracy: 0.2205 - val_loss: 2.3756\n",
      "Epoch 11/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2229 - loss: 2.3639 - val_accuracy: 0.2205 - val_loss: 2.3835\n",
      "Epoch 12/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2205 - loss: 2.3630 - val_accuracy: 0.2205 - val_loss: 2.3802\n",
      "Epoch 13/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2201 - loss: 2.3648 - val_accuracy: 0.2205 - val_loss: 2.3854\n",
      "Epoch 14/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2205 - loss: 2.3651 - val_accuracy: 0.2205 - val_loss: 2.3766\n",
      "Epoch 15/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2225 - loss: 2.3609 - val_accuracy: 0.2205 - val_loss: 2.3836\n",
      "Epoch 16/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.2218 - loss: 2.3625 - val_accuracy: 0.2205 - val_loss: 2.3838\n",
      "Epoch 17/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2218 - loss: 2.3634 - val_accuracy: 0.2205 - val_loss: 2.3780\n",
      "Epoch 18/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.2208 - loss: 2.3637 - val_accuracy: 0.2205 - val_loss: 2.3779\n",
      "Epoch 19/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2203 - loss: 2.3647 - val_accuracy: 0.2205 - val_loss: 2.3883\n",
      "Epoch 20/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.2214 - loss: 2.3594 - val_accuracy: 0.2205 - val_loss: 2.3800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18fb0dd4610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('data.csv')\n",
    "# Preprocesamiento de datos\n",
    "# Asegúrate de que los datos estén ordenados cronológicamente\n",
    "data['DATE OF OCCURRENCE'] = pd.to_datetime(data['DATE OF OCCURRENCE'], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "# data['DATE OF OCCURRENCE'] = pd.to_datetime(data['DATE OF OCCURRENCE'])\n",
    "data = data.sort_values(by='DATE OF OCCURRENCE')\n",
    "\n",
    "# Seleccionar características y etiquetas\n",
    "X = data[['DATE OF OCCURRENCE']].values\n",
    "y = data['PRIMARY DESCRIPTION']\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Ajustar las dimensiones para la entrada de ARNN (reshape)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "# Construir la red neuronal auto-recurrente (ARNN)\n",
    "model = Sequential([\n",
    " SimpleRNN(64, activation='relu', return_sequences=True),\n",
    " SimpleRNN(32, activation='relu'),\n",
    " Dense(len(y.unique()), activation='softmax')\n",
    "])\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convertir etiquetas de texto a valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Ajustar el modelo con las etiquetas numéricas\n",
    "model.fit(X_train, y_train_encoded, epochs=20, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "# Ajustar el modelo\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fcfe6-4477-4b1e-8038-e9a5290c41ca",
   "metadata": {},
   "source": [
    "## 4. Predicción de Valores Continuos con Redes Neuronales Profundas (DNN)\r",
    "*  Este ejercicio implica predecir la latitud y longitud de la ubicación de un crimen\r\n",
    "utilizando una red neuronal profunda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f59c8f53-e68c-466a-a601-4cb1429e9c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 1/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "\u001b[1m6469/6469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18fb944e4d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('data.csv')\n",
    "# Seleccionar características y etiquetas\n",
    "X = data[['X COORDINATE', 'Y COORDINATE']].values\n",
    "y_latitude = data['LATITUDE'].values\n",
    "y_longitude = data['LONGITUDE'].values\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_lat_train, y_lat_test, y_long_train, y_long_test = train_test_split( X, y_latitude, y_longitude, test_size=0.2, random_state=42)\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Construir la red neuronal profunda (DNN)\n",
    "# Construir la red neuronal profunda (DNN)\n",
    "model_lat = Sequential([\n",
    " Dense(64, activation='relu'),\n",
    " Dense(32, activation='relu'),\n",
    " Dense(1)\n",
    "])\n",
    "\n",
    "model_long = Sequential([\n",
    " Dense(64, activation='relu'),\n",
    " Dense(32, activation='relu'),\n",
    " Dense(1)\n",
    "])\n",
    "# Compilar el modelo\n",
    "model_lat.compile(optimizer='adam', loss='mse')\n",
    "model_long.compile(optimizer='adam', loss='mse')\n",
    "# Ajustar el modelo\n",
    "model_lat.fit(X_train, y_lat_train, epochs=20, batch_size=32, validation_data=(X_test, y_lat_test))\n",
    "model_long.fit(X_train, y_long_train, epochs=20, batch_size=32, validation_data=(X_test, y_long_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182343f9-b6d3-4303-8483-9e943c6762d4",
   "metadata": {},
   "source": [
    "## 5. Predicción de Texto usando Redes Neuronales Recurrentes (RNN)\r",
    "*  Este ejercicio implica predecir la descripción de un crimen basándose en su título\r\n",
    "y ubicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74ece264-b0c6-4ac6-a2b1-ca9ee03aaa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - accuracy: 0.1079 - loss: 3.7886 - val_accuracy: 0.1179 - val_loss: 3.6651\n",
      "Epoch 2/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1177 - loss: 3.6664 - val_accuracy: 0.1179 - val_loss: 3.6586\n",
      "Epoch 3/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1181 - loss: 3.6606 - val_accuracy: 0.1179 - val_loss: 3.6536\n",
      "Epoch 4/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - accuracy: 0.1175 - loss: 3.6608 - val_accuracy: 0.1179 - val_loss: 3.6558\n",
      "Epoch 5/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1179 - loss: 3.6630 - val_accuracy: 0.1179 - val_loss: 3.6543\n",
      "Epoch 6/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.1173 - loss: 3.6543 - val_accuracy: 0.1179 - val_loss: 3.6531\n",
      "Epoch 7/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1167 - loss: 3.6643 - val_accuracy: 0.1179 - val_loss: 3.6515\n",
      "Epoch 8/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1204 - loss: 3.6512 - val_accuracy: 0.1179 - val_loss: 3.6498\n",
      "Epoch 9/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1175 - loss: 3.6595 - val_accuracy: 0.1179 - val_loss: 3.6502\n",
      "Epoch 10/10\n",
      "\u001b[1m6436/6436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.1161 - loss: 3.6525 - val_accuracy: 0.1179 - val_loss: 3.6566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18fb64fc310>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Eliminar filas con valores nan\n",
    "data = data.dropna()\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "X_title = data['PRIMARY DESCRIPTION'].values\n",
    "X_location = data['LOCATION'].apply(lambda x: [float(coordinate) for coordinate in x[1:-1].split(', ')])  # Convertir coordenadas en lista de flotantes\n",
    "y = data['SECONDARY DESCRIPTION'].values\n",
    "\n",
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Separar datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_location, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar coordenadas geográficas\n",
    "def calculate_distance(coord):\n",
    "    # Supongamos que utilizamos el origen como punto de referencia\n",
    "    reference_point = [0.0, 0.0]\n",
    "    return np.linalg.norm(np.array(coord) - np.array(reference_point))\n",
    "\n",
    "X_train_normalized = np.array([calculate_distance(coord) for coord in X_train]).reshape(-1, 1)\n",
    "X_test_normalized = np.array([calculate_distance(coord) for coord in X_test]).reshape(-1, 1)\n",
    "\n",
    "# Modelo de texto\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ajustar el modelo\n",
    "model.fit(X_train_normalized, y_train, epochs=10, batch_size=32, validation_data=(X_test_normalized, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
